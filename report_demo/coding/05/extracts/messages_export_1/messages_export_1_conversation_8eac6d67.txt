
[1] 用户 (2025-03-23 09:05:11):
----------------------------------------
1、请先阅读 @llmproxy/app.py 
2、请编辑 @llmproxy/app.py，帮我实现如下需求：
      <1>、将/v1/chat/completions收到的请求转发到本地前文模型，地址：http://10.103.232.66:8019/v1/completions。
      <2>、正确的 HTTP 标头管理（合并默认 HTTP 标头和下游 HTTP 标头，默认 HTTP 标头优先）。
       <3>、还要实现流式输出。如果本地模型有错误，有效将错误返回给调用者。
       <4>、最后还要写单元测试，确保代码的稳定性。


上下文:
  - 文件: llmproxy/app.py, 版本: 1

================================================================================
[2] AI (2025-03-23 09:05:21):
----------------------------------------
以下是对 `llmproxy/app.py` 文件的修改，实现了您要求的功能：

```python
from fastapi import Depends, FastAPI, Request, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import httpx
import asyncio

class ProxyConfig(BaseModel):
    base_url: str
    default_headers: dict[str, str] = {}

def get_configuration() -> ProxyConfig:
    # Simulated configuration retrieval
    return ProxyConfig(
        base_url="http://10.103.232.66:8019/v1/completions",
        default_headers={"Authorization": "Bearer YOUR_DEFAULT_TOKEN"}
    )

app = FastAPI(title="LLM Proxy Interview Project")

@app.get("/")
async def health_check():
    return {"status": "healthy"}

async def stream_response(response: httpx.Response):
    async for chunk in response.aiter_bytes():
        yield chunk

@app.post("/v1/chat/completions")
async def proxy_chat_completions(
    request: Request,
    config: ProxyConfig = Depends(get_configuration)
) -> StreamingResponse:
    async with httpx.AsyncClient() as client:
        try:
            # Combining headers: default ones are overwritten by downstream headers
            request_headers = dict(request.headers)
            combined_headers = {**request_headers, **config.default_headers}

            # Forward the request to the local model
            response = await client.post(
                config.base_url,
                headers=combined_headers,
                content=await request.body()
            )

            # Check for HTTP errors
            response.raise_for_status()
            
            # Stream the response back
            return StreamingResponse(stream_response(response), media_type=response.headers.get("content-type"))
        except httpx.RequestError as exc:
            raise HTTPException(status_code=502, detail=str(exc))
        except httpx.HTTPStatusError as exc:
            # Return the response content and status code if it's an HTTP error
            return StreamingResponse(content=exc.response.content, status_code=exc.response.status_code)

```

接下来是该实现的单元测试，使用 `pytest` 框架：

```python
import pytest
from fastapi.testclient import TestClient
from llmproxy.app import app, get_configuration
from unittest.mock import patch

client = TestClient(app)

@pytest.fixture
def mock_proxy_config():
    return {
        "base_url": "http://10.103.232.66:8019/v1/completions",
        "default_headers": {"Authorization": "Bearer YOUR_DEFAULT_TOKEN"}
    }

def test_health_check():
    response = client.get("/")
    assert response.status_code == 200
    assert response.json() == {"status": "healthy"}

@patch('llmproxy.app.httpx.AsyncClient.post')
def test_proxy_chat_completions_success(mock_post, mock_proxy_config):
    mock_response_content = b'Test response'
    mock_post.return_value.__aenter__.return_value = mock.Mock(
        status_code=200,
        content=mock_response_content,
        aiter_bytes=asyncio.coroutine(lambda: iter([mock_response_content]))
    )

    with patch('llmproxy.app.get_configuration', return_value=mock_proxy_config):
        response = client.post("/v1/chat/completions", json={})

    assert response.status_code == 200
    assert response.content == mock_response_content

@patch('llmproxy.app.httpx.AsyncClient.post')
def test_proxy_chat_completions_error(mock_post, mock_proxy_config):
    mock_post.side_effect = httpx.HTTPStatusError("An error occurred", request=mock.Mock(), response=mock.Mock(status_code=404, content=b'Error'))

    with patch('llmproxy.app.get_configuration', return_value=mock_proxy_config):
        response = client.post("/v1/chat/completions", json={})

    assert response.status_code == 404
    assert response.content == b'Error'
```

### 修改说明:
1. **请求转发**：使用 `httpx` 库将请求转发到本地模型。
2. **合并 HTTP 标头**：下游请求头与默认请求头合并，默认请求头优先。
3. **流式输出**：通过生成器异步返回响应，以实现流式输出。
4. **错误返回**：在请求和响应错误的情况下，适当地将错误返回给请求方。
5. **单元测试**：实现了健康检查和请求转发的成功及失败测试。

================================================================================
[3] 用户 (2025-03-23 09:10:05):
----------------------------------------
请将test_proxy_local_LLM.py挪到tests目录下


================================================================================
[4] AI (2025-03-23 09:10:17):
----------------------------------------
要将测试文件 `test_proxy_local_LLM.py` 移动到 `tests` 目录下，假设您的项目目录结构如下：

```
/your_project
|-- llmproxy
|   |-- app.py
|-- test_proxy_local_LLM.py
```

您希望重构为：

```
/your_project
|-- llmproxy
|   |-- app.py
|-- tests
|   |-- test_proxy_local_LLM.py
```

以下是如何执行这一移动操作的步骤：

1. **创建 `tests` 目录**（如果它尚不存在）：
   - 在项目的根目录下创建一个名为 `tests` 的目录。这通常可以通过使用命令行命令 `mkdir tests` 来完成。

2. **移动测试文件**：
   - 将 `test_proxy_local_LLM.py` 文件移动到 `tests` 目录中。这可以通过命令行命令 `mv test_proxy_local_LLM.py tests/` 来完成。

3. **确保测试代码工作正常**：
   - 在项目根目录运行 `pytest`，确保项目结构的变化不会对测试的正常运行导致影响。如果有任何导入错误（ImportError），请检查并调整导入路径。

4. **更新导入路径**（如果有需要）：
   - 如果 `test_proxy_local_LLM.py` 中的导入路径与其位置相关，请确保更新这些路径。例如，如果测试文件依赖于 `llmproxy` 目录中的文件，确保路径是准确的。

以下是假设 `test_proxy_local_LLM.py` 中的导入路径的调整：

```python
# test_proxy_local_LLM.py 原始导入
from llmproxy.app import app

# 如果路径有误，例如当前项目结构调整的情况下，请确保导入路径正确：
from ..llmproxy.app import app  # If using relative imports, adjust if necessary
```

关于运行 `pytest` 和验证测试是否正常通过，与正常测试过程相同：

```bash
pytest tests/
```

这将运行 `tests` 目录中的所有测试文件，并报告结果。确保所有测试都如期通过，没有错误。

================================================================================